100)
    {
      snippet = part.Text[..100] + "...";
    }

    table.AddRow(citation.DocumentId, part.PartitionNumber.ToString(), part.SectionNumber.ToString(), part.Relevance.ToString("P2"), snippet);
  }
}

table.Expand();
console.Write(table);
console. WriteLine();



    Enter fullscreen mode
    


    Exit fullscreen mode
    








This produces a formatted table resembling the following image:



As you can see, each match will have a document, partition, section within that partition, relevance score, and some associated text. Individual tags and source URLs will also be available. Note how document names are not mandatory, but Kernel Memory generates its own random Ids if you don't provide an id yourself.

You can use SearchAsync to manually identify the most relevant documents and pieces of documents from your vector store. This can be useful for providing semantic search capabilities across your site, or for identifying text to inject into prompts as a form of Retrieval Augmetnation Generation (RAG). However, if you're working with RAG, there's a chance you might be better off using the AskAsync method instead, as we'll see next.


  
  
  Question answering with KernelMemory and LLM


If your end goal is to provide a reply to the user from a query they sent you, you should consider using Kernel Memory's AskAsync method.

AskAsync uses the text model to summarize the result of a search and provide that string back to the user.


