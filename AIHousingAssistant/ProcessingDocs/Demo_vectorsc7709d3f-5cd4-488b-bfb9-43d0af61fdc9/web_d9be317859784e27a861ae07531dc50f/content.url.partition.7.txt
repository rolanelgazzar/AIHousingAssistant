, Azure AI Search, Postgres, Redis, or others.

Likewise, Kernel Memory supports a wide range of LLMs and other ingestion data sources including OpenAI, Anthropic, ONNX, and even locally-running Ollama models.

This last point has me particularly excited because I can now use locally hosted LLMs and on-network vector storage solutions to ingest and search documents without needing to worry about data leaving my network or per-usage cloud hosting costs. This opens up new usage scenarios for me for experimentation, workshops at conferences, and business scenarios.

Let's drill into a larger Kernel Memory app and see how it flows.


  
  
  Creating a Kernel Memory instance in C


Through the rest of this article we'll walk through a small C# console application from start to finish. This project is available on GitHub if you'd like to clone it locally and experiment with it as well, though you'll need to provide your own API keys.

Next we use some fairly ordinary C# code using the Microsoft.Extensions.Configuration mechanism for reading settings:



IConfiguration config = new ConfigurationBuilder()
    .AddJsonFile("appsettings.json", optional: true, reloadOnChange: false)
    .AddEnvironmentVariables()
    .AddUserSecrets&lt;Program&gt;()
    .AddCommandLine(args)
    .Build();
DocSearchDemoSettings settings = config.Get&lt;DocSearchDemoSettings&gt;()!;



    Enter fullscreen mode
    


    Exit fullscreen mode
    







